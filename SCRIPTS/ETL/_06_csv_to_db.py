"""
ETL STEP 6: Load data from GOLD layer to databases
Part of ETL pipeline: LOAD step (CSV → DB)

Loads data into:
1. Supabase (cloud database)
2. SQLite local database (WNDMNGR.APP)

This script always uses UPSERT mode (safe).
To wipe data first, run _05_wipe_database.py before this script,
or use: invoke csv-to-db --truncate
"""
import os
import sys
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv
from supabase import create_client, Client
from loguru import logger
import ssl
import urllib3
import httpx
import sqlite3

# Disable SSL warnings for corporate proxy
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

# Load environment
load_dotenv()

# Paths
BASE_DIR = Path(__file__).parent.parent.parent
GOLD_DIR = BASE_DIR / 'DATA' / 'GOLD'

# Local SQLite database for WNDMNGR.APP
WNDMNGR_APP_DIR = BASE_DIR.parent / 'WNDMNGR.APP'
SQLITE_DB_PATH = WNDMNGR_APP_DIR / 'DATA' / 'windmanager.db'

# Supabase credentials
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_API_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    logger.error("Missing SUPABASE_URL or SUPABASE_API_KEY in .env")
    sys.exit(1)

# Ensure URL ends with /
if not SUPABASE_URL.endswith('/'):
    SUPABASE_URL = SUPABASE_URL + '/'

# Create httpx client with SSL verification disabled (corporate proxy)
http_client = httpx.Client(verify=False)

# Create Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Patch PostgREST session to use custom httpx client
if hasattr(supabase.postgrest, 'session'):
    supabase.postgrest.session = http_client

# Load order (respects foreign key dependencies)
LOAD_ORDER = [
    # References (no dependencies)
    ('farm_types', 'farm_types.csv'),
    ('company_roles', 'company_roles.csv'),
    ('person_roles', 'person_roles.csv'),

    # Entities (no dependencies)
    ('companies', 'companies.csv'),
    ('persons', 'persons.csv'),
    ('ice_detection_systems', 'ice_detection_systems.csv'),

    # Farms (no dependencies)
    ('farms', 'farms.csv'),

    # Substations (depend on farms) - MUST be loaded before WTG
    ('substations', 'substations.csv'),

    # WTG (depend on farms and substations)
    ('wind_turbine_generators', 'wind_turbine_generators.csv'),

    # Employees (depend on companies and persons)
    ('employees', 'employees.csv'),

    # Relationships (depend on farms, companies, persons)
    ('farm_company_roles', 'farm_company_roles.csv'),
    ('farm_referents', 'farm_referents.csv'),

    # Lookups (all depend on farms)
    ('farm_administrations', 'farm_administrations.csv'),
    ('farm_environmental_installations', 'farm_environmental_installations.csv'),
    ('farm_financial_guarantees', 'farm_financial_guarantees.csv'),
    ('farm_locations', 'farm_locations.csv'),
    ('farm_om_contracts', 'farm_om_contracts.csv'),
    ('farm_tcma_contracts', 'farm_tcma_contracts.csv'),
    ('farm_statuses', 'farm_statuses.csv'),
    ('farm_substation_details', 'farm_substation_details.csv'),
    ('farm_turbine_details', 'farm_turbine_details.csv'),
    ('farm_ice_detection_systems', 'farm_ice_detection_systems.csv'),
    # Note: farm_tariffs, farm_actual_performances, farm_target_performances,
    # and farm_electrical_delegations are not generated by silver_to_gold.py
]

# Map table names to the column to use for deletion (must be non-null)
DELETE_KEYS = {
    'farm_types': 'id',
    'company_roles': 'id',
    'person_roles': 'id',
    'companies': 'uuid',
    'persons': 'uuid',
    'ice_detection_systems': 'uuid',
    'farms': 'uuid',
    'wind_turbine_generators': 'uuid',
    'substations': 'uuid',
    'employees': 'uuid',

    # All other farm_* tables usually have farm_uuid
    'farm_company_roles': 'farm_uuid',
    'farm_referents': 'farm_uuid',
    'farm_administrations': 'farm_uuid',
    'farm_environmental_installations': 'farm_uuid',
    'farm_financial_guarantees': 'farm_uuid',
    'farm_locations': 'farm_uuid',
    'farm_om_contracts': 'farm_uuid',
    'farm_tcma_contracts': 'farm_uuid',
    'farm_statuses': 'farm_uuid',
    'farm_substation_details': 'farm_uuid',
    'farm_turbine_details': 'wind_farm_uuid',  # Special case: uses wind_farm_uuid
    'farm_ice_detection_systems': 'farm_uuid',
}


def load_table(table_name: str, csv_file: str):
    """Load a CSV file into a Supabase table

    Returns: 'success', 'warning', or 'failed'
    """

    csv_path = GOLD_DIR / csv_file

    if not csv_path.exists():
        logger.warning(f"  ⊘ {table_name}: File not found ({csv_file}), skipping")
        return 'warning'

    logger.info(f"Loading {table_name} to Supabase...")

    try:
        # Read CSV
        df = pd.read_csv(csv_path)

        if df.empty:
            logger.warning(f"  → Empty file, skipping")
            return 'warning'

        # Replace Inf/-Inf and NaN with None (for proper NULL handling and JSON compliance)
        import numpy as np
        df = df.replace([np.inf, -np.inf, np.nan], None)

        # Convert 1.0/0.0 to boolean for specific boolean columns
        boolean_columns = {
            'farm_administrations': ['has_remit_subscription'],
            'ice_detection_systems': ['automatic_stop', 'automatic_restart']
        }
        if table_name in boolean_columns:
            for col in boolean_columns[table_name]:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: True if x == 1.0 or x == 1 else (False if x == 0.0 or x == 0 else None))

        # Convert to dict records
        records = df.to_dict('records')

        logger.info(f"  → {len(records)} rows to insert")

        # Insert in batches (Supabase API has limits)
        batch_size = 1000
        total_inserted = 0

        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]

            try:
                response = supabase.table(table_name).upsert(batch).execute()  # type: ignore
                total_inserted += len(batch)
                logger.info(f"  → Batch {i // batch_size + 1}: {len(batch)} rows")
            except Exception as e:
                logger.error(f"  → Batch error: {str(e)[:200]}")
                # Continue with next batch

        logger.success(f"  ✓ {table_name}: {total_inserted} rows loaded to Supabase")
        return 'success'

    except Exception as e:
        logger.error(f"  ✗ Error loading {table_name}: {str(e)[:200]}")
        return 'failed'


def load_table_sqlite(table_name: str, csv_file: str, conn: sqlite3.Connection):
    """Load a CSV file into a SQLite table (local WNDMNGR.APP database)

    Returns: 'success', 'warning', or 'failed'
    """

    csv_path = GOLD_DIR / csv_file

    if not csv_path.exists():
        return 'warning'  # Already logged warning in load_table()

    try:
        # Read CSV
        df = pd.read_csv(csv_path)

        if df.empty:
            return 'warning'

        # Replace Inf/-Inf and NaN with None
        import numpy as np
        df = df.replace([np.inf, -np.inf, np.nan], None)

        # Load to SQLite (replace existing data)
        df.to_sql(table_name, conn, if_exists='replace', index=False)

        logger.success(f"  ✓ {table_name}: {len(df)} rows loaded to SQLite")
        return 'success'

    except Exception as e:
        logger.error(f"  ✗ Error loading {table_name} to SQLite: {str(e)[:200]}")
        return 'failed'


def main():
    """Load all GOLD data to Supabase and SQLite"""

    # Configure logger to force colors
    logger.remove()
    logger.add(sys.stderr, colorize=True, format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>")

    logger.info("=" * 80)
    logger.info("ETL STEP 6: CSV → DB (Load GOLD data to databases)")
    logger.info("=" * 80)
    logger.info("Targets:")
    logger.info("  1. Supabase (cloud)")
    logger.info(f"  2. SQLite local ({SQLITE_DB_PATH})")
    logger.info("")
    logger.info("Mode: UPSERT (safe - updates existing, inserts new)")
    logger.info(f"Tables to load: {len(LOAD_ORDER)}")
    logger.info("")

    # Connect to SQLite
    sqlite_conn = None
    if SQLITE_DB_PATH.exists():
        try:
            sqlite_conn = sqlite3.connect(SQLITE_DB_PATH)
            logger.info(f"✓ Connected to SQLite: {SQLITE_DB_PATH}")
        except Exception as e:
            logger.warning(f"Could not connect to SQLite: {str(e)[:200]}")
    else:
        logger.warning(f"SQLite database not found: {SQLITE_DB_PATH}")

    logger.info("")

    success_count = 0
    warning_count = 0
    failed_count = 0

    for table_name, csv_file in LOAD_ORDER:
        # Load to Supabase
        supabase_status = load_table(table_name, csv_file)

        # Load to SQLite (if connected)
        sqlite_status = 'success'
        if sqlite_conn:
            sqlite_status = load_table_sqlite(table_name, csv_file, sqlite_conn)

        # Count worst status (failed > warning > success)
        if supabase_status == 'failed' or sqlite_status == 'failed':
            failed_count += 1
        elif supabase_status == 'warning' or sqlite_status == 'warning':
            warning_count += 1
        else:
            success_count += 1

    # Close SQLite connection
    if sqlite_conn:
        sqlite_conn.commit()
        sqlite_conn.close()
        logger.info("")
        logger.info("✓ SQLite connection closed")

    logger.info("")
    logger.info("=" * 80)
    logger.info("LOAD COMPLETE")
    logger.info("=" * 80)

    # Display summary with colors
    if success_count > 0:
        logger.success(f"✓ Success: {success_count}/{len(LOAD_ORDER)}")
    if warning_count > 0:
        logger.warning(f"⚠ Warning: {warning_count}/{len(LOAD_ORDER)} (empty/missing tables)")
    if failed_count > 0:
        logger.error(f"✗ Failed: {failed_count}/{len(LOAD_ORDER)}")

    logger.info("=" * 80)

    if failed_count > 0:
        sys.exit(1)


if __name__ == '__main__':
    main()
